[
    {
        "course_code": "INFR11125",
        "course_title": "Accelerated Natural Language Processing",
        "course_summary": "This course synthesizes ideas from linguistics and computer science to provide students with a fast-paced introduction to the field of natural language processing. We cover a range of foundational concepts, theoretical and computational models of language, and linguistic phenomena. We focus on what makes automatic processing of language unique and challenging: its statistical properties, complex structure, and pervasive ambiguity. We use English as the primary exemplar throughout, but also discuss similarities and differences to other languages, and the implications for computational models. \n\nAs we progress from lower levels of linguistic structure (words and morphemes) through syntax to semantics and discourse, we cover formal models and algorithms for representing and analysing these different types of structure (e.g., n-gram models, HMMs, dependency parsing, dynamic programming, and word embeddings). These methods form the conceptual foundation for understanding state-of-the-art approaches, which are covered in depth in the Semester 2 follow-on course NLU+. We also introduce methodological and ethical considerations (e.g., evaluation, data collection, algorithmic bias) that are important for research in the field.",
        "course_desc": "The course follows a roughly 3-part structure, following the progression of linguistic structure. Each part introduces linguistic phenomena and associated NLP models and algorithms. Interspersed throughout we discuss potential applications (e.g., sentiment analysis, text classification, QA systems) and methodological topics (e.g., evaluation, annotation, ethical concerns). \n\nPart I: Words \n* Inflectional and derivational morphology \n* Finite state methods and Regular expressions \n* Bag-of-Words models and their applications \n* Word Classes and Parts of speech \n* Sequence Models (n-gram and Hidden Markov models, smoothing) \n* The Viterbi algorithm, Forward Backward, EM \n\nPart II: Syntax \n* Syntactic Concepts (e.g., constituency, subcategorisation, bounded and unbounded dependencies, feature representations) \n* Analysis in CFG - Greedy algorithms---Shift-reduce parsing \n* Divide-and-conquer algorithms---CKY \n* Lexicalised grammar formalisms (e.g., CCG, dependency grammar) \n* Statistical parsing (PCFGs, dependency parsing) \n\nPart III: Semantics and Discourse \n* Logical semantics and compositionality \n* Semantic derivations in grammar \n* Lexical Semantics (e.g., word embeddings, word senses, semantic roles) \n* Discourse (e.g., anaphora)",
        "course_credits": 20,
        "semester": "Semester 1",
        "prerequisites": "",
        "prohibited_combinations": [
            "Foundations of Natural Language Processing (INFR09028) OR\nFoundations of Natural Language Processing (INFR10078)"
        ],
        "other_requirements": "",
        "learning_outcomes": "identify, construct, and analyse examples of different kinds of ambiguity in natural language (e.g., ambiguity in part-of-speech, word sense, syntactic attachment). Explain how ambiguity presents a problem for computational analysis, and some of the ways it can be addressed.\ndescribe and apply standard sequence and classification models; describe parsing and search algorithms for different levels of analysis (e.g. morphology, syntax, and semantics) and simulate each algorithm step-by-step with pen and paper.\nfor a range of NLP tasks, outline a processing pipeline for that task, including standard data sets, models, algorithms, and evaluation methods. Given a particular pipeline or part of the pipeline, identify potential strengths and weaknesses of the suggested dataset/method (including both technical and ethical issues, where appropriate), and provide examples to illustrate.\nimplement parts of the NLP pipeline with the help of appropriate support code and/or tools. Evaluate and interpret the results of implemented methods on natural language data sets."
    }
]